---
title: "Ordinal Regression Demo"
author: "Alex Kale"
date: "6/2/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(brms)
library(tidybayes)
library(gganimate)
knitr::opts_chunk$set(echo = TRUE)
```

## Ordinal regression in brms

Let's load some data from Experiment 2 of _Truncating the Y-Axis: Threat or Menace?_ The experiment compares whether different visualization designs signaling that the axis has been truncated can counteract distortions in perceived differences between bars in a bar chart. Participants judge the magnitude of differences (i.e., effect size) on a 5-point Likert-style scale.

The data is at https://osf.io/5pjgb/, and the paper is at https://arxiv.org/pdf/1907.02035.pdf.

```{r}
data <- read_csv("cleanrows2.csv")
```

Let's preprocess our data for the purpose of modeling. This mostly involves renaming some variables and/or casting some of them as factors. We also want to drop training trials from our data as Correll et al. did in the paper.

```{r}
data <- data %>%
  mutate(
    vis = as.factor(visType),
    # truncation = as.factor(truncationLevel),
    # slope = as.factor(slope),
    complexity = as.factor(dataSize),
    subject = as.factor(id)) %>%
  rename(
    response = qSeverity,
    truncation = truncationLevel
  ) %>%
  filter(is.na(training))
```

We're going to fit an ordinal regression model using the brms package. You can read about this kind of model and brms at https://osf.io/gyfj7/download. Basically, the model assumes that the ordinal response on a K-point scale is generated by partitioning a latent (unobserved) continuous variable using K - 1 thresholds. The latent scale represents the participant's mental representation of perceived effect size in arbitrary units. The thresholds separating adjacent response categories are intercepts in our model, and their location on the latent scale is modified by each fixed and random effect parameter in our model.

The model specification we've chosen will enable us to make inferences about the impact of different visualization designs and different levels of axis truncation, controlling for the actual maginitude of the difference participants are judging, the number of bars shown, and individual variability in responses. Note that in addition to modeling the threshold locations on the latent scale, we are also modeling the variability in the latent scale (i.e., ability to discriminate the underlying signal), allowing it to vary per participant.

We'll fit the model using the default priors in brms.

```{r}
m1 <- brm(
  formula = bf(
    response ~ vis + truncation*slope + complexity + (1|subject),
    disc ~ (1|subject)),
  family = cumulative("probit"),
  chains = 2,
  cores = 2,
  iter = 2500,
  warmup = 1000,
  data = data,
  control = list(adapt_delta = 0.99),
  file = "ordinal-model1")
```

Let's check some diagnostics.

In this summary table, we want to see Rhat values very close to 1.0 and Bulk_ESS in the thousands for proper inference. *Rhat* is a measure of chain mixing. We want to know that our chains are not getting stuck in local maxima of the parameter space in terms of log joint posterior densitity (i.e., what is being optimized in the sampling algorithm). When chains are getting stuck at local maxima, this is usually a sign that our parameter space is "lumpy" in its geometry, and we should probably reparameterize our model.

*Bulk_ESS* is effective sample size. This tells us how many functionally independent samples from the posterior we actually have. For joint posteriors with strong correlation between parameters, this can we quite a lot lower than the number of MCMC iterations after warmup. We can bump Bulk_ESS higher by running our model fitting process for more samples.

```{r}
summary(m1)
```

*Trace plots* help us look at chain mixing in greater detail. This can be helpful for diagnosing exactly which parameters the sampler is getting stuck on. 

```{r}
plot(m1)
```

*Pairs plots* show us correlation between pairs of parameters. Perfectly correlated parameters are usually bad news. This means that our model cannot differentiate the effect of two redundant parameters, often because of issues of identifiability. This perfect correlation between parameters is called *multicollinearity*, and it usually means we need to reparameterize our model.

I usually separate these out into coherent sets of parameters since these plots don't scale well. 

```{r}
pairs(m1, pars = c("b_Intercept[1]","b_Intercept[2]","b_Intercept[3]", "b_Intercept[4]", "b_disc_Intercept"), exact_match = TRUE)
```

Some of these intercepts look pretty strongly correlated, but it doesn't seem to have caused any other issues with the model fit.

```{r}
pairs(m1, pars = c("sd_subject__Intercept", "sd_subject__disc_Intercept"), exact_match = TRUE)
```

```{r}
pairs(m1, pars = c("b_truncation", "b_slope"))
```

```{r}
pairs(m1, pars = c("b_vis", "b_complexity"))
```

Let's compare the empirical distribution of responses to the distribution predicted by the model.

Here's the empirical distribution.

```{r}
data %>%
  mutate(rating = ordered(response, levels = c("5","4","3","2","1"))) %>%
  ggplot(aes(x = vis, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ truncation)
```

Here's the predicted distribution. Note that the model produces many draws (i.e., samples) consistituting a predictive distribution for each observation in the data set. In order to make the counts match, we take the model's median response on each trial. 

```{r}
data %>%
  add_predicted_draws(m1, seed = 14) %>%
  group_by(vis, truncation, index, subject) %>%
  summarize(rating = ordered(as.factor(median(as.numeric(.prediction))), levels =c("5","4","3","2","1"))) %>%
  ggplot(aes(x = vis, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ truncation)
```

Notice how the average model prediction for each trial errs on the side of underestimating responses. This is just an artifact of taking the median of the distribution of predictions for each trial.

Now, instead of taking a median, let's plot the predictive uncertainty of the model as animated hypothetical outcome plots (HOPs).

```{r}
spec <- data %>%
  add_predicted_draws(m1, seed = 14, n = 50) %>%
  mutate(rating = ordered(.prediction, levels = c("5","4","3","2","1"))) %>%
  ggplot(aes(x = vis, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ truncation) +
    transition_manual(.draw)

animate(spec, fps = 2.5)
```

This shows us in detail how uncertain the model is about the distribution of responses in each condition. However, it does not give us a sense of inferential uncertainty. One common way that researchers tend to make inferences with Likert-style responses is to model differences in average response. We can do the same thing with the output of the ordinal regression model, and we can do so without assuming that these discrete responses are the outcome of an untransformed linear model (e.g., ANOVA).

Let's find the average response in each condition, conditioning on the average user (i.e., no longer including random effects in our predictions). Note that now we are taking the average rating in each condition for each draw, giving us a distribution of inferential uncertainty about average responses (i.e., a sampling distribution of average response) rather than a wider distribution of predictive uncertainty about non-aggregated responses.

```{r}
data %>%
  add_predicted_draws(m1, seed = 14, re_formula = NA) %>%
  group_by(vis, truncation, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(.prediction))) %>%
  ggplot(aes(x = vis, y = rating)) +
    stat_eye() +
    theme_minimal() +
    coord_cartesian(ylim = c(1.0, 3.5)) +
    facet_grid(. ~ truncation)
```

Let's look at contrasts for the effect of visualization design

```{r}
data %>%
  add_predicted_draws(m1, seed = 14, re_formula = NA) %>%
  group_by(vis, .draw) %>%
  summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
  compare_levels(rating, by = vis) %>%
  rename(diff_in_rating = rating) %>%
  ggplot(aes(x = diff_in_rating, y = vis)) +
    stat_halfeyeh() +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()
```

One nice thing about Bayesian analysis is that we can say that we do not find a reliable difference between visualization conditions without all of that "failing to reject the null" business.

And let's also look at contrasts for the effect of axis truncation.

```{r}
data %>%
  add_predicted_draws(m1, seed = 14, re_formula = NA) %>%
  group_by(truncation, .draw) %>%
  summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
  compare_levels(rating, by = truncation) %>%
  rename(diff_in_rating = rating) %>%
  ggplot(aes(x = diff_in_rating, y = truncation)) +
    stat_halfeyeh() +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()
```

These differences are really big, but we could see also see this in a few of the previous charts. Axis limits have a pretty robust influence on perceived effect size.

##Troubleshooting

Let's take a model that doesn't fit and try to diagnose what's wrong. This model uses a more complicated interaction term than the last one by assuming that complexity (number of bars) interacts with visualization design, truncation, and slope (effect size).

```{r}
m2 <- brm(
  formula = bf(
    response ~ vis*truncation*slope*complexity + (1|subject),
    disc ~ (1|subject)),
  family = cumulative("probit"),
  chains = 2,
  cores = 2,
  iter = 2500,
  warmup = 1000,
  data = data,
  control = list(adapt_delta = 0.99),
  file = "ordinal-model2")
```

We can see from the summary that we had divergent chains (Rhat > 1.1).

```{r}
summary(m2)
```

Let's take a look at these divergent chains as trace plots.

```{r}
plot(m2)
```

We can see that the chains are wandering around without converging. This makes me think that we could improve our model fit with some regularization (narrower priors). 

Sometimes (not in this case) chains will get stuck sampling a single parameter value repeatedly. This usually signals issues with identifiability, suggesting that our model is mis-specified and should be reparameterized.

Let's take a look at the default priors we've been using before we create our own.

```{r}
get_prior(
  formula = bf(
    response ~ vis*truncation*slope*complexity + (1|subject),
    disc ~ (1|subject)),
  family = cumulative("probit"),
  data = data)
```

We've got a bunch of slope parameters because of all of the interaction effects in our model. Let's look at the first model (the one that fit) for comparison. 

```{r}
get_prior(
  formula = bf(
    response ~ vis*truncation*slope + complexity + (1|subject),
    disc ~ (1|subject)),
  family = cumulative("probit"),
  data = data)
```

That's a difference of 11 parameters from changing a `+` to a `*` in our model specification! This is one reason why it's smart to build models up incrementally in order to understand more complex models in terms of simpler ones, a process called model expansion.

Let's try placeing some narrower priors on these slope parameters. This should make it easier for the sampler to hone in on a specific region of parameter space.

```{r}
m3 <- brm(
  formula = bf(
    response ~ vis*truncation*slope*complexity + (1|subject),
    disc ~ (1|subject)),
  family = cumulative("probit"),
  prior = prior(normal(0, 0.5), class = b),
  chains = 2,
  cores = 2,
  iter = 2500,
  warmup = 1000,
  data = data,
  control = list(adapt_delta = 0.99),
  file = "ordinal-model3")
```

Let's check out model fit.

```{r}
summary(m3)
```

```{r}
plot(m3)
```

This looks much better! Why did changing the prior work? 

Imagine the sampler as a marble dropped into a bowl and sweeping out a path along the bowl's surface. In this common metafor, the marble is the sampler, the surface of the bowl is parameter space, and discrete locations on marble's path are our posterior draws. When we add many parameters to a model, as we did when we added multiple interactions to our model, we make an n-dimensional surface that can become "lumpy" with local extrema making it difficult for the model to converge. By setting regularizing priors we smooth the surface of the parameter space, making it easier for chains to converge.

Another way of thinking about this is that adding parameters to a model entails an assumption that there are more sources of variation. As we add more sources of variation, we need to assume that each individual parameter is going to contribute less variance to the posterior distribution. Otherwise, we tell the sampler to explore an overdispersed parameter space, leading to the wandering chains we saw before.

Let's compare the empirical distribution of responses to the distribution predicted by the model.

Here's the empirical distribution.

```{r}
data %>%
  mutate(rating = ordered(response, levels = c("5","4","3","2","1"))) %>%
  ggplot(aes(x = vis, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ truncation)
```

Here's the posterior predictive distribution as HOPs.

```{r}
spec <- data %>%
  add_predicted_draws(m3, seed = 14, n = 50) %>%
  mutate(rating = ordered(.prediction, levels = c("5","4","3","2","1"))) %>%
  ggplot(aes(x = vis, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ truncation) +
    transition_manual(.draw)

animate(spec, fps = 2.5)
```

This looks pretty similar to the simpler model we fit previously. Let's compare the two models that fit well using leave-one-out (llo) crossvalidation, which estimates out-of-sample predictive validity as expected log posterior density (elpd). As a rule of thumb, smaller values of elpd reflect a better fit.

```{r}
loo(m1, m3)
```

It looks like the more complex model (m3) fits slightly better, even after we adjust for overfitting. It is common practice to run a handful of model, expanding from simpler to more complex specifications, and compare them all using posterior predictive visualizations and metrics like loo cross-validation.
